### **Method 1: Landmark-Weighted Attention Loss**
*Simplest and most direct approach*

**Concept**: Add an auxiliary loss that encourages attention maps to have higher values at landmark regions.

**Implementation**:
```python
# Attention regularization loss
def landmark_attention_loss(attention_maps, landmark_masks):
    """
    attention_maps: (B, M, H, W) - learned attention maps
    landmark_masks: (B, 1, H, W) - binary/soft masks for important regions
    """
    # Upsample attention to image size if needed
    B, M, AH, AW = attention_maps.shape
    _, _, MH, MW = landmark_masks.shape
    
    if AH != MH or AW != MW:
        attention_maps = F.interpolate(attention_maps, size=(MH, MW), mode='bilinear')
    
    # Compute attention concentration on landmarks
    # Average across all attention heads
    attention_avg = attention_maps.mean(dim=1, keepdim=True)  # (B, 1, H, W)
    
    # L2 loss to encourage attention on landmarks
    landmark_region_attention = attention_avg * landmark_masks
    non_landmark_region_attention = attention_avg * (1 - landmark_masks)
    
    # Maximize attention on landmarks, minimize on non-landmarks
    loss = -landmark_region_attention.mean() + 0.5 * non_landmark_region_attention.mean()
    
    return loss
```

**Data Preparation**:
- **Input**: 478 Mediapipe landmarks â†’ Select key regions:
  - Eyes: landmarks 33, 133, 159, 145, 362, 385, 380, 386 (eye contours)
  - Nose: landmarks 1, 2, 98, 327 (nose bridge and tip)
  - Mouth: landmarks 61, 291, 0, 17, 84, 314 (mouth contours)
- **Create binary mask**: Set pixels within radius (e.g., 10-20 pixels) of these landmarks to 1
- **Store**: `landmark_masks` as (B, 1, H, W) tensor alongside images

**Pros**: Simple, interpretable, low computational cost  
**Cons**: Treats all attention heads equally, may be too rigid

Ráº¥t hay â€” hÃ m `landmark_attention_loss()` trong Ä‘oáº¡n code nÃ y lÃ  má»™t **hÃ m loss tÃ¹y chá»‰nh** Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ **hÆ°á»›ng sá»± chÃº Ã½ (attention)** cá»§a máº¡ng há»c sÃ¢u **táº­p trung vÃ o cÃ¡c vÃ¹ng Ä‘áº·c trÆ°ng (landmarks)** cá»§a Ä‘á»‘i tÆ°á»£ng (vÃ­ dá»¥: máº¯t, mÅ©i, miá»‡ng trÃªn khuÃ´n máº·t).
MÃ¬nh sáº½ giáº£i thÃ­ch chi tiáº¿t tá»«ng bÆ°á»›c vÃ  cÃ´ng thá»©c toÃ¡n há»c tÆ°Æ¡ng á»©ng ğŸ‘‡

---

## ğŸ§© 1. Má»¥c tiÃªu cá»§a Landmark Attention Loss

HÃ m loss nÃ y Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ **hÆ°á»›ng dáº«n mÃ´ hÃ¬nh há»c cÃ¡ch táº­p trung vÃ o vÃ¹ng landmark** (nhá»¯ng vÃ¹ng cÃ³ thÃ´ng tin quan trá»ng) trong áº£nh.
Cá»¥ thá»ƒ:

* **TÄƒng attention** trong vÃ¹ng *landmark* (vÃ¹ng cÃ³ mask = 1)
* **Giáº£m attention** trong vÃ¹ng *non-landmark* (vÃ¹ng cÃ²n láº¡i, mask = 0)

---

## ğŸ§® 2. CÃ´ng thá»©c toÃ¡n há»c

### a. KÃ½ hiá»‡u

| KÃ½ hiá»‡u                                                 | Ã nghÄ©a                                                              |
| ------------------------------------------------------- | -------------------------------------------------------------------- |
| ( A \in \mathbb{R}^{B \times M \times H_A \times W_A} ) | attention maps cá»§a mÃ´ hÃ¬nh (B batch, M báº£n Ä‘á»“ attention)             |
| ( L \in \mathbb{R}^{B \times 1 \times H_L \times W_L} ) | landmark mask (vÃ¹ng Ä‘áº·c trÆ°ng cá»§a Ä‘á»‘i tÆ°á»£ng, giÃ¡ trá»‹ âˆˆ [0,1])        |
| ( \tilde{A} )                                           | attention map sau khi Ä‘Æ°á»£c ná»™i suy vá» cÃ¹ng kÃ­ch thÆ°á»›c vá»›i mask ( L ) |
| ( \bar{A} = \frac{1}{M} \sum_{m=1}^{M} \tilde{A}_m )    | trung bÃ¬nh attention trÃªn táº¥t cáº£ cÃ¡c head                            |
| ( L' = 1 - L )                                          | vÃ¹ng khÃ´ng pháº£i landmark                                             |

---

### b. TÃ­nh **má»©c Ä‘á»™ attention trung bÃ¬nh** trong vÃ¹ng landmark vÃ  non-landmark

Ta tÃ­nh tá»•ng attention trong tá»«ng vÃ¹ng vÃ  chia cho diá»‡n tÃ­ch vÃ¹ng Ä‘Ã³:

[
S_\text{landmark} = \frac{\sum_{i,j} \bar{A}*{ij} \cdot L*{ij}}{\sum_{i,j} L_{ij} + \varepsilon}
]

[
S_\text{non} = \frac{\sum_{i,j} \bar{A}*{ij} \cdot (1 - L*{ij})}{\sum_{i,j} (1 - L_{ij}) + \varepsilon}
]

---

### c. Äá»‹nh nghÄ©a **hÃ m loss**

Ta muá»‘n **maximize ( S_\text{landmark} )** vÃ  **minimize ( S_\text{non} )**.
Thay vÃ¬ trá»±c tiáº¿p dÃ¹ng subtraction (dá»… gÃ¢y báº¥t á»•n), tÃ¡c giáº£ dÃ¹ng **tá»· lá»‡ (ratio)** Ä‘á»ƒ Ä‘áº£m báº£o á»•n Ä‘á»‹nh:

[
\text{loss} = -\log \left( \frac{S_\text{landmark}}{S_\text{landmark} + S_\text{non} + \varepsilon} \right)
]

Hoáº·c tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i:
[
\boxed{L_{\text{landmark}} = -\log \frac{S_\text{landmark}}{S_\text{landmark} + S_\text{non} + \varepsilon}}
]

* Náº¿u ( S_\text{landmark} ) lá»›n hÆ¡n ( S_\text{non} ) â‡’ loss nhá» â‡’ mÃ´ hÃ¬nh há»c Ä‘Ãºng hÆ°á»›ng.
* Náº¿u attention bá»‹ lá»‡ch (non-landmark cÃ³ nhiá»u attention) â‡’ loss tÄƒng â‡’ mÃ´ hÃ¬nh bá»‹ pháº¡t.

---

## ğŸ§  3. Giáº£i thÃ­ch trá»±c quan

### ğŸ“ VÃ­ dá»¥ trá»±c quan:

Giáº£ sá»­ áº£nh khuÃ´n máº·t cÃ³ mask vÃ¹ng â€œmáº¯t, mÅ©i, miá»‡ngâ€.
MÃ´ hÃ¬nh sinh ra attention map táº­p trung lung tung trÃªn tÃ³c, ná»n, quáº§n Ã¡o...
â†’ `landmark_attention_loss` sáº½ tÄƒng cao.

MÃ´ hÃ¬nh sáº½ dáº§n há»c cÃ¡ch:

* Giáº£m attention á»Ÿ vÃ¹ng khÃ´ng quan trá»ng (non-landmark).
* TÄƒng attention á»Ÿ vÃ¹ng mask (landmark).

---

## âš™ï¸ 4. Code tÆ°Æ¡ng á»©ng (diá»…n giáº£i tá»«ng bÆ°á»›c)

```python
# Resize attention map vá» cÃ¹ng kÃ­ch thÆ°á»›c vá»›i landmark mask
attention_maps_resized = F.interpolate(attention_maps, size=(MH, MW), mode='bilinear')

# Trung bÃ¬nh táº¥t cáº£ cÃ¡c attention head
attention_avg = attention_maps_resized.mean(dim=1, keepdim=True)

# TÃ­nh attention trung bÃ¬nh trong vÃ¹ng landmark / non-landmark
landmark_score = (attention_avg * landmark_masks).sum(...) / (landmark_masks.sum(...) + 1e-6)
non_landmark_score = (attention_avg * (1 - landmark_masks)).sum(...) / ((1 - landmark_masks).sum(...) + 1e-6)

# HÃ m loss theo tá»· lá»‡ (ratio)
loss = -torch.log(landmark_score / (landmark_score + non_landmark_score + 1e-6))
```

---

## ğŸ§¾ 5. Diá»…n giáº£i Ã½ nghÄ©a gradient

* Gradient cá»§a loss nÃ y sáº½ **Ä‘áº©y attention map tÄƒng giÃ¡ trá»‹ á»Ÿ vÃ¹ng landmark**,
  vÃ¬ viá»‡c tÄƒng ( S_\text{landmark} ) lÃ m giáº£m giÃ¡ trá»‹ loss.
* Äá»“ng thá»i **giáº£m attention á»Ÿ vÃ¹ng khÃ¡c** vÃ¬ ( S_\text{non} ) náº±m á»Ÿ máº«u sá»‘.

Káº¿t quáº£: attention maps há»c Ä‘Æ°á»£c sáº½ **phÃ¹ há»£p hÆ¡n vá»›i cáº¥u trÃºc Ä‘á»‘i tÆ°á»£ng tháº­t**, giÃºp model nháº­n dáº¡ng tá»‘t hÆ¡n.

---

## ğŸ“š 6. Tá»•ng káº¿t

| ThÃ nh pháº§n          | Ã nghÄ©a                                                                                                                     |
| ------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| **Input**           | attention maps vÃ  landmark masks                                                                                            |
| **Má»¥c tiÃªu**        | Ã©p máº¡ng chÃº Ã½ nhiá»u hÆ¡n vÃ o vÃ¹ng landmark                                                                                   |
| **CÃ´ng thá»©c chÃ­nh** | ( L = -\log \frac{S_\text{landmark}}{S_\text{landmark} + S_\text{non} + \varepsilon} )                                      |
| **Hiá»‡u quáº£**        | cáº£i thiá»‡n kháº£ nÄƒng há»c Ä‘áº·c trÆ°ng hÃ¬nh dáº¡ng, há»— trá»£ fine-grained recognition (vÃ­ dá»¥ nháº­n dáº¡ng khuÃ´n máº·t, chim, xe hÆ¡i, v.v.) |

---

Náº¿u báº¡n muá»‘n mÃ¬nh **viáº¿t láº¡i cÃ´ng thá»©c loss nÃ y báº±ng kÃ½ hiá»‡u LaTeX chi tiáº¿t** (vá»›i giáº£i thÃ­ch Ä‘áº¡o hÃ m hoáº·c biá»ƒu Ä‘á»“ trá»±c quan), mÃ¬nh cÃ³ thá»ƒ thÃªm pháº§n Ä‘Ã³ â€” báº¡n cÃ³ muá»‘n mÃ¬nh má»Ÿ rá»™ng khÃ´ng?


Ráº¥t hay â€” cÃ¢u há»i cá»§a báº¡n Ä‘i Ä‘Ãºng trá»ng tÃ¢m ğŸ’¡
ChÃºng ta sáº½ phÃ¢n tÃ­ch **hÃ m loss tá»•ng thá»ƒ (`batch_loss`)** trong Ä‘oáº¡n code, vÃ  **hiá»ƒu chÃ­nh xÃ¡c landmark attention loss (`lm_loss`) Ä‘Æ°á»£c â€œtÄƒng cÆ°á»ngâ€ hay â€œgá»™p vÃ oâ€ nhÆ° tháº¿ nÃ o** trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.

---

## ğŸ§© 1. Tá»•ng quan: cáº¥u trÃºc cá»§a `batch_loss`

Trong hÃ m `train()`, pháº§n quan trá»ng lÃ :

```python
batch_loss = (
    cross_entropy_loss(y_pred_raw, y) / 3.
    + cross_entropy_loss(y_pred_aux, y_aux) * 3. / 3.
    + cross_entropy_loss(y_pred_aug, y_aug) * 2. / 3.
    + center_loss(feature_matrix, feature_center_batch)
)
```

Sau Ä‘Ã³ (náº¿u cÃ³ landmark guidance):

```python
if use_landmark_loss:
    lm_loss = landmark_attention_loss(full_attention_maps, landmark_masks)
    batch_loss = batch_loss + config.landmark_loss_weight * lm_loss
```

---

## ğŸ§® 2. CÃ´ng thá»©c tá»•ng quÃ¡t cá»§a `batch_loss`

Ta cÃ³ thá»ƒ viáº¿t gá»n láº¡i thÃ nh:

[
\boxed{
L_\text{total}
= \frac{1}{3} L_\text{raw}

* 1 \cdot L_\text{aux}
* \frac{2}{3} L_\text{aug}
* L_\text{center}
* \lambda_\text{lm} , L_\text{landmark}
  }
  ]

trong Ä‘Ã³:

| KÃ½ hiá»‡u                                                                                                | Ã nghÄ©a                                    |
| ------------------------------------------------------------------------------------------------------ | ------------------------------------------ |
| ( L_\text{raw} = CE(y_{\text{pred_raw}}, y) )                                                          | cross-entropy loss cá»§a Ä‘áº§u ra gá»‘c          |
| ( L_\text{aux} = CE(y_{\text{pred_aux}}, y_{\text{aux}}) )                                             | loss cá»§a Ä‘áº§u ra phá»¥ (auxiliary classifier) |
| ( L_\text{aug} = CE(y_{\text{pred_aug}}, y_{\text{aug}}) )                                             | loss khi dÃ¹ng áº£nh augment (crop/drop)      |
| ( L_\text{center} = \text{CenterLoss}(f, c) )                                                          | Ã©p feature vector gáº§n tÃ¢m lá»›p tÆ°Æ¡ng á»©ng    |
| ( L_\text{landmark} = -\log \frac{S_\text{landmark}}{S_\text{landmark} + S_\text{non} + \varepsilon} ) | landmark attention loss                    |
| ( \lambda_\text{lm} = \text{config.landmark_loss_weight} )                                             | há»‡ sá»‘ trá»ng sá»‘ cho landmark loss           |

---

## âš™ï¸ 3. Vai trÃ² cá»§a tá»«ng thÃ nh pháº§n

| ThÃ nh pháº§n                     | Má»¥c tiÃªu                                      | áº¢nh hÆ°á»Ÿng Ä‘áº¿n há»c                                       |
| ------------------------------ | --------------------------------------------- | ------------------------------------------------------- |
| **CrossEntropy (raw/aux/aug)** | Há»c phÃ¢n loáº¡i Ä‘Ãºng nhÃ£n                       | HÆ°á»›ng gradient dá»±a trÃªn lá»—i dá»± Ä‘oÃ¡n                     |
| **Center Loss**                | LÃ m Ä‘áº·c trÆ°ng (feature) cá»§a cÃ¹ng lá»›p gáº§n nhau | GiÃºp tÄƒng tÃ­nh phÃ¢n biá»‡t trong khÃ´ng gian embedding     |
| **Landmark Attention Loss**    | HÆ°á»›ng attention cá»§a máº¡ng vÃ o vÃ¹ng cÃ³ landmark | Cáº£i thiá»‡n kháº£ nÄƒng há»c khÃ´ng gian thá»‹ giÃ¡c (tÄƒng focus) |

---

## ğŸ§  4. Landmark loss â€œtÄƒng cÆ°á»ngâ€ batch_loss nhÆ° tháº¿ nÃ o?

Khi ta thÃªm dÃ²ng nÃ y:

```python
batch_loss = batch_loss + config.landmark_loss_weight * lm_loss
```

tá»©c lÃ  gradient tá»•ng (qua phÃ©p Ä‘áº¡o hÃ m ngÆ°á»£c `backward()`) sáº½ lÃ :

[
\nabla_\theta L_\text{total}
= \nabla_\theta (L_\text{cls} + \lambda_\text{lm} L_\text{landmark})
= \nabla_\theta L_\text{cls} + \lambda_\text{lm} \nabla_\theta L_\text{landmark}
]

â†’ **Tá»©c lÃ  landmark loss táº¡o thÃªm má»™t thÃ nh pháº§n gradient**, Ä‘iá»u chá»‰nh hÆ°á»›ng cáº­p nháº­t trá»ng sá»‘ cá»§a máº¡ng sao cho:

* CÃ¡c layer attention **há»c táº­p trung hÆ¡n vÃ o vÃ¹ng mask** (vÃ¬ Ä‘áº¡o hÃ m cá»§a ( L_\text{landmark} ) Ä‘áº©y attention tÄƒng á»Ÿ vÃ¹ng cÃ³ mask = 1),
* Trong khi cÃ¡c layer classifier **váº«n Ä‘Æ°á»£c huáº¥n luyá»‡n nhÆ° bÃ¬nh thÆ°á»ng** (tá»« cross entropy vÃ  center loss).

---

## ğŸ“Š 5. Trá»±c giÃ¡c vá» â€œtÄƒng cÆ°á»ngâ€ (enhancement)

| TrÆ°á»›c khi thÃªm ( L_\text{landmark} )                                 | Sau khi thÃªm ( L_\text{landmark} )                                   |
| -------------------------------------------------------------------- | -------------------------------------------------------------------- |
| Attention cÃ³ thá»ƒ há»c lá»™n xá»™n, táº­p trung sai vÃ¹ng (background, noise) | Attention Ä‘Æ°á»£c Ä‘iá»u chá»‰nh hÆ°á»›ng Ä‘áº¿n cÃ¡c vÃ¹ng quan trá»ng              |
| Cross-entropy chá»‰ há»c dá»±a vÃ o nhÃ£n tá»•ng thá»ƒ                          | Landmark loss thÃªm **rÃ ng buá»™c khÃ´ng gian** giÃºp há»c táº­p á»•n Ä‘á»‹nh hÆ¡n |
| Feature space cÃ³ thá»ƒ bá»‹ nhiá»…u                                        | Feature map mang Ã½ nghÄ©a thá»‹ giÃ¡c rÃµ rÃ ng hÆ¡n                        |

---

## ğŸ§© 6. áº¢nh hÆ°á»Ÿng cá»§a há»‡ sá»‘ `landmark_loss_weight`

Tham sá»‘ nÃ y Ä‘Æ°á»£c gá»i lÃ  **há»‡ sá»‘ tÄƒng cÆ°á»ng (Î»)**:

```python
batch_loss = batch_loss + config.landmark_loss_weight * lm_loss
```

* Náº¿u `Î»` quÃ¡ nhá» â†’ landmark guidance khÃ´ng áº£nh hÆ°á»Ÿng Ä‘Ã¡ng ká»ƒ.
* Náº¿u `Î»` quÃ¡ lá»›n â†’ mÃ´ hÃ¬nh cÃ³ thá»ƒ **quÃ¡ táº­p trung vÃ o mask**, bá» quÃªn kháº£ nÄƒng phÃ¢n loáº¡i tá»•ng thá»ƒ.

ğŸ‘‰ ThÃ´ng thÆ°á»ng, `Î»` Ä‘Æ°á»£c chá»n qua **validation**, vÃ­ dá»¥ `Î» = 0.1`, `0.5`, hoáº·c `1.0`.

---

## ğŸ” 7. VÃ­ dá»¥ minh há»a (sá»‘ giáº£ Ä‘á»‹nh)

Giáº£ sá»­:

* CE losses = 0.8
* Center loss = 0.1
* Landmark loss = 0.5
* `Î» = 0.3`

Khi Ä‘Ã³:

[
L_\text{total} = 0.8 + 0.1 + 0.3 \times 0.5 = 1.05
]

Landmark loss chiáº¿m **15%** áº£nh hÆ°á»Ÿng, vá»«a Ä‘á»§ Ä‘á»ƒ hÆ°á»›ng attention mÃ  khÃ´ng lÃ m lá»‡ch má»¥c tiÃªu phÃ¢n loáº¡i chÃ­nh.

---

## âœ… 8. Káº¿t luáº­n

**Landmark Attention Loss** khÃ´ng thay tháº¿ cÃ¡c loss khÃ¡c â€” nÃ³ **bá»• sung thÃªm má»™t rÃ ng buá»™c khÃ´ng gian**, giÃºp mÃ´ hÃ¬nh há»c â€œnhÃ¬n Ä‘Ãºng chá»—â€.

CÃ´ng thá»©c tá»•ng quÃ¡t cá»§a loss toÃ n pháº§n:

[
\boxed{
L_\text{total}
= \alpha_1 L_\text{raw}

* \alpha_2 L_\text{aux}
* \alpha_3 L_\text{aug}
* L_\text{center}
* \lambda_\text{lm} L_\text{landmark}
  }
  ]

vá»›i ( \lambda_\text{lm} ) lÃ  trá»ng sá»‘ Ä‘iá»u chá»‰nh má»©c â€œtÄƒng cÆ°á»ngâ€ cá»§a landmark loss.

---

Báº¡n cÃ³ muá»‘n mÃ¬nh **váº½ sÆ¡ Ä‘á»“ trá»±c quan** thá»ƒ hiá»‡n cÃ¡ch tá»«ng thÃ nh pháº§n loss tÃ¡c Ä‘á»™ng lÃªn cÃ¡c pháº§n cá»§a máº¡ng (classification head, feature extractor, attention maps, landmark guidance)?
SÆ¡ Ä‘á»“ Ä‘Ã³ sáº½ giÃºp báº¡n tháº¥y rÃµ Ä‘Æ°á»ng Ä‘i cá»§a gradient trong pipeline huáº¥n luyá»‡n.
